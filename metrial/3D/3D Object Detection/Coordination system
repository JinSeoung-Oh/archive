기본 용어 정리
World Coordination system : Real world 상에서 물체가 위치하는 좌표계

Camera Coordination system : 물체가 카메라 렌즈 상에 맺힐 때의 좌표계를 의미. 깊게 들어가면 좀 더 복잡하지만, 카메라 렌즈 위의 좌표계라고 이해하면 쉬움

Image Coordination system :  2D 이미지 좌표계를 의미

Open image-20220412-004452.png

각 좌표계 사이의 관계
위의 각 좌표계 사이의 관계 이미지를 통해 2D image는 Camera coordinates 상 위의 좌표 (X,Y,Z)를 XY 평면 위로 정사형한 결과 값이라고 정의할 수 있음

따라서 3D object detection model의 핵심은 (X,Y,Z)를 XY로 정사형 할 때 사용된 Projection Matrix를 구하는 것

참고 : https://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf

카메라 모델
위의 기본 용어 정리에서 언급한 Projection Matrix를 구하기 위하여 다양한 카메라 모델들이 이용됨

이 중에서 가장 자주 사용 되는 것이 Pinhole Camera model로 가장 쉽게 설명하자면 종이 한 가운데에 구멍을 뚫고 그 구멍을 통해 다른 사물을 바라보는 모델이라고 생각하면 편함

Pinhole Camera model
Open image-20220412-005722.png

Pinhole Camera model은 F_c와 Principal point(C_x,C_y) 사이의 거리인 focal length가 1이라는 가정하에 있는 모델

Open image-20220412-010525.png

Camera model에서는 Camera 자체가 가지는(기종에 따라서 다른) distortion matrix가 Projection matrix에 포함 되는데, pinhole model은 이 distortion matrix에서 radial distortion과 tangential distortion가 Focal length가 1이기 때문에 0이라고 가정 됨

Pinhole model에서의 projection matrix는 아래와 같이 정의 됨

Open image-20220412-011201.png

참고 : [SLAM] Opencv Camera model 정리 · Jinyong , Pinhole Camera Model | HediVision 

따라서, 카메라 모델에서 각 point 간의 관계는, P'' = K[R t]P'로 정의 될 수 있음 (아래 그림과 같이)

Open image-20220412-020120.png

Forward Projection 
Ex. P' = (U,V,W)이면 P''는 (X,Y,Z)이어야 함. 만약 P'=(X,Y,Z)이면 P''=(x,y,1) 혹은 (u,v,1)이어야 함

Open image-20220412-020343.png

Backward Projection
Backward projection도 가능하며, 이 경우 좀 더 복잡한 선형대수학이 필요함

 

Internal camera parameters(intrinsic camera parameters)
이때, K는 Internal camera parameters(intrinsic camera parameters)로 고정된 특정 값임

Pinhole model과 일반 카메라 모델과의 차이점은 K matrix의 (2,1) 값이 0이 아닌 특정 값을 가진다는 것인데 이 때, 이 값(s)은 카메라 렌즈로 인하여 발생하는 lens distortion 값임

External camera parameters(Extrinsic camera parameters)
Open image-20220412-012123.png

External camera parameters는 위 그림과 같이 정의 되는 두 개의 Matrix로 구성됨

[R t]는 External parameters(Extrinsic camera parameters)로 고정 된 값이 아니라 사진을 찍는 매 순간, 각 obj마다 다른 값을 가지는 matrix임. (R = rotated matrix , T = translation matrix) → 사진을 찍은 사람만이 알 수 있는 값이라고 이해하면 쉬움)

따라서, 3D object detection model에서 Projection matrix를 구하기 위해서는 Internal and External camera parameters에 대한 정보가 반드시 필요 →  3D object detection model의 dataset들은 camera calibration 파일 안에 이 정보들을 담고 있음

3D object detection 방법론
3D object detection 방법에는 다양한 방법론이 존재

      1. point cloud based approach

      2. 3D modeling based approach(ex. with Mesh model)

      3. Convert 2D bbox to 3D box

      4. etc

 대표적으로 사용되는 방법은 1~3에 해당하는 방법론으로 이 중 1,2번의 경우 기본 방법 자체는 비슷한 모습을 보임

1,2번의 방법론의 경우 일반적으로 3D space(bired’s eye view map(BEV map))를 구성 하고 해당 Space에서 bboxing 작업 후에 이를 2D image로 projection을 하는 방식을 채택 (Projection matrix가 필요한 이유)

3번의 경우 Object의 direction에 대한 각도를 구한 뒤, 이를 이용하여 2D bbox 내에 존재하는 64개의 3D voxcel 구조의 후보 군 중에서 1개의 후보를 고르는 방식임 → 참고 : Convert 2D bbox to 3D bbox / Convert 2D image to BEV map - research 

3D object detection model에서 Object direction의 중요도
3D object detection model에서는 3D voxcel 구조 외에도 Object의 방향에 대한 정보 또한 매우 중요

Open image-20220412-013324.png

위 복셀 구조 상에서 노란 색 영역에 Car의 front 부분이 오든 back 부분이 오든 2D image 상에서 복셀 구조 자체는 변하지 않음

따라서 복셀 구조만으로는 2D image 내에서 object의 방향성을 구할 수 없음

2D image는 카메라 좌표(X,Y,Z)를 XY 평면 위로 projection 한 결과 값으로 정의 될 수 있기 때문에, Z축에서의 rotated angle인 yaw만을 고려 하게 됨 → 참고 : roll, pitch, yaw란 

다만, 어느 평면 위로 Projection을 했느냐에 따라서 고려하는 값들이 달라지게 됨 (ex. XY평면이면 yaw, XZ평면이면 pitch(y축에서의 rotated angle)

 

Yaw에는 Global yaw과 Local yaw이 존재 (roll과 pitch에서도 해당 내용 적용 됨)

Open image-20220412-014050.png

1 ) Global Yaw는 Egocentric direction이라고도 하며 각각의 obj를 중심으로 진행 방향을 의미하는데,

아래의 사진 오른쪽에 나타나는 자동차는 차선을 따라 직선으로 움직이고 있음 → 이 차선을 따라 움직이는

방향을 Global Yaw 혹은 Egocentric direction이라고 부름

Open image-20220412-014133.png

 

2. Local Yaw는 Allocentric direction 혹은 Observation angle이라고 불리우며, Camera point로부터 obj

center point를 잇는 직선으로 정의 될 수 있음 → 위 사진에서 왼쪽 crop된 이미지들을 보면 obj의 진

행 방향들이 Camera point와의 거리에 따라서 달라짐을 볼 수 있음

 

*Global Yaw는 2D image coordination system에서는 고정되고, Local Yaw는 카메라와의 각도에 따라서

달라지지만, Camera coordination system에서는 Local Yaw가 고정되고 Global Yaw가 변하게 됨

(Camera coordination system은 Camera point를 기준으로 생성되는 좌표계이기 때문에 Camera point와

obj의 관계에 의해서 정의되는 Local Yaw가 고정 값을 지니게 됨)

 

위에서 설명한 Global Yaw와 Local Yaw 값을 이용하여 Object의 direction을 구할 수 있음

Open image-20220412-014334.png

위의 그림에서 설명하는 공식은 하나의 방법일 뿐이며, 이 외에도 다양한 방법으로 Global yaw와 Local yaw를 이용하여 Object의 방향성을 구할 수 있음

위 그림은 BEV map( bired’s eye view map) 상에서 각도를 구했기 때문에, 각 값들은 XY 평면 위에서의 값이 아닌 XZ 평면 위에서의 값들임

